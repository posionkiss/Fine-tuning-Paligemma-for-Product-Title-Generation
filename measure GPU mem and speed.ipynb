{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-08T14:03:07.979962Z",
     "start_time": "2025-04-08T14:03:06.186879Z"
    }
   },
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from modeling_gemma import KVCache\n",
    "from processing_paligemma import PaliGemmaProcessor, LabelProcessor\n",
    "from utils import move_inputs_to_device\n",
    "from prepare_data import ImageInstructionOutputDataset, update_tokenizer, update_embeddings, prepare_dataset\n",
    "from utils import *\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def compute_training_memory(model, input_size, optimizer_type='adam', device='cuda'):\n",
    "    \"\"\"\n",
    "    计算神经网络训练时的显存峰值使用量\n",
    "\n",
    "    参数:\n",
    "        model (nn.Module): 神经网络模型\n",
    "        input_size (tuple): 输入张量尺寸 (batch_size, ...)\n",
    "        optimizer_type (str): 优化器类型，支持 'adam' 或 'sgd'\n",
    "        device (str): 计算设备 ('cuda' 或 'cpu')\n",
    "\n",
    "    返回:\n",
    "        int: 峰值显存占用量（字节）\n",
    "    \"\"\"\n",
    "    # 确保使用GPU\n",
    "    if device != 'cuda':\n",
    "        raise ValueError(\"显存计算需要CUDA设备\")\n",
    "\n",
    "    # 将模型移至GPU\n",
    "    model = model.to(device)\n",
    "\n",
    "    # 生成虚拟输入数据和标签\n",
    "    dummy_input = torch.randn(*input_size).to(device)\n",
    "    dummy_target = torch.randint(0, 10, (input_size[0],)).to(device)\n",
    "\n",
    "    # 定义损失函数\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # 重置显存统计\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    try:\n",
    "        # 前向传播\n",
    "        outputs = model(dummy_input)\n",
    "        loss = criterion(outputs, dummy_target)\n",
    "\n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "\n",
    "        # 创建优化器并执行一步更新\n",
    "        if optimizer_type.lower() == 'adam':\n",
    "            optimizer = torch.optim.Adam(model.parameters())\n",
    "        elif optimizer_type.lower() == 'sgd':\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "        else:\n",
    "            raise ValueError(\"支持的优化器类型: 'adam' 或 'sgd'\")\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # 获取峰值显存占用\n",
    "        peak_memory = torch.cuda.max_memory_allocated(device=device)\n",
    "\n",
    "    finally:\n",
    "        # 清理内存\n",
    "        del dummy_input, dummy_target, outputs, loss\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return peak_memory\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T14:03:25.752856Z",
     "start_time": "2025-04-08T14:03:09.841115Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = 'cpu'\n",
    "print(\"Device in use: \", device)\n",
    "model_path = 'paligemma'\n",
    "\n",
    "print(f\"Loading model\")\n",
    "tokenizer_modified = True\n",
    "rank = 16  # LoRA rank\n",
    "model, tokenizer = load_hf_model(model_path, freeze_vision=0)\n",
    "vocab_size = len(tokenizer)\n",
    "\n",
    "# 计算显存使用量\n",
    "input_shape = (224, 224, 3)  # batch_size=32\n",
    "peak_mem = compute_training_memory(model, input_shape)\n",
    "\n",
    "# 打印结果\n",
    "print(f\"训练所需峰值显存: {peak_mem / 1024**2:.2f} MB\")\n",
    "print(f\"详细组成:\")\n",
    "print(f\"- 模型参数: {sum(p.numel() for p in model.parameters())} 个参数\")\n",
    "print(f\"- 输入尺寸: {input_shape}\")"
   ],
   "id": "99ba0bb51d7492bc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device in use:  cuda\n",
      "Loading model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7c5503080e2e457f9db6991238eb329d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\He\\lib\\site-packages\\peft\\tuners\\tuners_utils.py:550: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['model.embed_tokens', 'lm_head'] are part of the adapter. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. See for example https://github.com/huggingface/peft/issues/2018.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'outputs' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 48\u001B[0m, in \u001B[0;36mcompute_training_memory\u001B[1;34m(model, input_size, optimizer_type, device)\u001B[0m\n\u001B[0;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     47\u001B[0m     \u001B[38;5;66;03m# 前向传播\u001B[39;00m\n\u001B[1;32m---> 48\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdummy_input\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     49\u001B[0m     loss \u001B[38;5;241m=\u001B[39m criterion(outputs, dummy_target)\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\He\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\He\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32mD:\\He\\NUS311\\FYP\\Paligemma\\modeling_gemma.py:495\u001B[0m, in \u001B[0;36mPaliGemmaForConditionalGeneration.forward\u001B[1;34m(self, input_ids, pixel_values, attention_mask, kv_cache)\u001B[0m\n\u001B[0;32m    489\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    490\u001B[0m             input_ids\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    491\u001B[0m             pixel_values\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    492\u001B[0m             attention_mask\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, kv_cache\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    493\u001B[0m     \u001B[38;5;66;03m# assert torch.all(attention_mask == 1), 'The input cannot be padded'\u001B[39;00m\n\u001B[1;32m--> 495\u001B[0m     inputs_embeds \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlanguage_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_input_embeddings\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    497\u001B[0m     selected_image_feature, attention_weight_list \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvision_tower(pixel_values)\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\He\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\He\\lib\\site-packages\\torch\\nn\\modules\\module.py:1582\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1580\u001B[0m     args \u001B[38;5;241m=\u001B[39m bw_hook\u001B[38;5;241m.\u001B[39msetup_input_hook(args)\n\u001B[1;32m-> 1582\u001B[0m result \u001B[38;5;241m=\u001B[39m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1583\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks:\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\He\\lib\\site-packages\\peft\\tuners\\lora\\layer.py:982\u001B[0m, in \u001B[0;36mEmbedding.forward\u001B[1;34m(self, x, *args, **kwargs)\u001B[0m\n\u001B[0;32m    981\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 982\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbase_layer(x, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    983\u001B[0m     torch_result_dtype \u001B[38;5;241m=\u001B[39m result\u001B[38;5;241m.\u001B[39mdtype\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\He\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\He\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\He\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:163\u001B[0m, in \u001B[0;36mEmbedding.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    162\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 163\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membedding\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    164\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_norm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    165\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnorm_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscale_grad_by_freq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msparse\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\He\\lib\\site-packages\\torch\\nn\\functional.py:2264\u001B[0m, in \u001B[0;36membedding\u001B[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001B[0m\n\u001B[0;32m   2263\u001B[0m     _no_grad_embedding_renorm_(weight, \u001B[38;5;28minput\u001B[39m, max_norm, norm_type)\n\u001B[1;32m-> 2264\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membedding\u001B[49m\u001B[43m(\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscale_grad_by_freq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msparse\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mUnboundLocalError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 18\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;66;03m# 计算显存使用量\u001B[39;00m\n\u001B[0;32m     17\u001B[0m input_shape \u001B[38;5;241m=\u001B[39m (\u001B[38;5;241m32\u001B[39m, \u001B[38;5;241m1000\u001B[39m)  \u001B[38;5;66;03m# batch_size=32\u001B[39;00m\n\u001B[1;32m---> 18\u001B[0m peak_mem \u001B[38;5;241m=\u001B[39m \u001B[43mcompute_training_memory\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_shape\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;66;03m# 打印结果\u001B[39;00m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m训练所需峰值显存: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpeak_mem\u001B[38;5;250m \u001B[39m\u001B[38;5;241m/\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1024\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m2\u001B[39m\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.2f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m MB\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[4], line 69\u001B[0m, in \u001B[0;36mcompute_training_memory\u001B[1;34m(model, input_size, optimizer_type, device)\u001B[0m\n\u001B[0;32m     65\u001B[0m     peak_memory \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mmax_memory_allocated(device\u001B[38;5;241m=\u001B[39mdevice)\n\u001B[0;32m     67\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;66;03m# 清理内存\u001B[39;00m\n\u001B[1;32m---> 69\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m dummy_input, dummy_target, outputs, loss\n\u001B[0;32m     70\u001B[0m     torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mempty_cache()\n\u001B[0;32m     72\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m peak_memory\n",
      "\u001B[1;31mUnboundLocalError\u001B[0m: local variable 'outputs' referenced before assignment"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "280c355b8ff5ae44"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
