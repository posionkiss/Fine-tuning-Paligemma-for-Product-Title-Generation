{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44495daf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T15:56:52.114713Z",
     "start_time": "2025-03-04T15:56:47.794252Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device in use:  cuda\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from modeling_gemma import KVCache\n",
    "from processing_paligemma import PaliGemmaProcessor, LabelProcessor\n",
    "from utils import move_inputs_to_device\n",
    "from prepare_data import ImageInstructionOutputDataset, update_tokenizer, update_embeddings, prepare_dataset\n",
    "from utils import *\n",
    "from datetime import datetime\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = 'cpu'\n",
    "print(\"Device in use: \", device)\n",
    "model_path = 'paligemma'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c2c6f1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T15:57:28.681052Z",
     "start_time": "2025-03-04T15:56:52.242869Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e01d5693ef047599b81a37082a08a50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You shouldn't move a model that is dispatched using accelerate hooks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 18,432,000 || all params: 2,527,094,784 || trainable%: 0.7294\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading model\")\n",
    "tokenizer_modified = True\n",
    "rank = 16  # LoRA rank\n",
    "# model, tokenizer = load_hf_model(model_path, device, 1)\n",
    "bit=4\n",
    "model, tokenizer = load_hf_model(model_path, freeze_vision=0,bit=bit)\n",
    "try_round = 'color_3'\n",
    "# model = apply_lora(model, device, rank=rank)\n",
    "# model = model.half()\n",
    "model = model.to(device).eval()\n",
    "vocab_size = len(tokenizer)\n",
    "cp_epoch = 39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d35d023",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T15:57:29.130911Z",
     "start_time": "2025-03-04T15:57:28.693054Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update embeddings\n"
     ]
    }
   ],
   "source": [
    "update_embeddings(model, model.vocab_size, vocab_size, device)\n",
    "num_image_tokens = model.config.vision_config.num_image_tokens\n",
    "image_size = model.config.vision_config.image_size\n",
    "processor = PaliGemmaProcessor(tokenizer, num_image_tokens, (image_size, image_size))\n",
    "label_processor = LabelProcessor(tokenizer)\n",
    "stop_token = processor.tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af46fc49",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T15:58:17.634698Z",
     "start_time": "2025-03-04T15:57:29.178312Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data augmenting...\n",
      "35551/35552\n",
      "Viewing the set into batch_size=6\n",
      "Data augmenting...\n",
      "4443/4444\n",
      "Viewing the set into batch_size=6\n"
     ]
    }
   ],
   "source": [
    "batch_size = 6\n",
    "training_set, validation_set = prepare_dataset(bs=batch_size)\n",
    "colors_dict = list(np.unique(training_set.baseColour))\n",
    "colors_dict = {color: i for i, color in enumerate(colors_dict)}\n",
    "\n",
    "cate_dict = list(np.unique(training_set.subCategory))\n",
    "cate_dict = {cate: i for i, cate in enumerate(cate_dict)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0d6b28ee",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float16\n",
      "torch.float16\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float16\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float16\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float16\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float16\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float16\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float16\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float16\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float16\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float16\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float16\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float16\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float16\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float16\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float16\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float16\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float16\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float16\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float16\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float16\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float16\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float16\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float16\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float16\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float16\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float16\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float16\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float16\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float16\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float16\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float16\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float16\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float16\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float16\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float16\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float16\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float16\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float16\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float16\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float16\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float16\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float16\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float16\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float16\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float16\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float16\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float16\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float16\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float16\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float16\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float16\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float16\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float16\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float16\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int8\n",
      "torch.int8\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "for n,m in model.named_modules():\n",
    "    if hasattr(m, 'weight'):\n",
    "#         print(m.weight.dtype)\n",
    "        m.weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb9311e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = f'my_model/5/model_rank16_epoch_38_.pth'\n",
    "model.load_state_dict(torch.load(checkpoint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a37463",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T16:03:31.157732400Z",
     "start_time": "2025-03-04T15:59:45.129236Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/scipy/__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.4)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training on classifying colors:\n",
      "Start at epoch 39\n",
      "\n",
      "Shuffle training set\n",
      "Viewing the set into batch_size=6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch39\tIter2790\t1\tPredict Loss:2.94371\r"
     ]
    }
   ],
   "source": [
    "'''\n",
    "多loss训练\n",
    "'''\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "try_round = '5'\n",
    "\n",
    "\n",
    "# for name, m in model.named_parameters():  # 只更新后两层\n",
    "#     if ('vision_tower.vision_model.encoder.layers.26' in name or 'vision_tower.vision_model.encoder.layers.25' in name or 'post_layernorm' in name) and 'original' not in name:\n",
    "#         m.requires_grad = True\n",
    "#     else:\n",
    "#         m.requires_grad = False\n",
    "\n",
    "class GlobalClassifyingHead(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_dim=1152,  # 输入特征维度\n",
    "                 compress_dim=512,  # 特征压缩维度\n",
    "                 num_cates=17):  # 类别数\n",
    "        super().__init__()\n",
    "        # 特征压缩与全局融合\n",
    "        self.feature_fusion = nn.Sequential(\n",
    "            nn.Linear(in_dim, compress_dim),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        # 注意力聚合模块（自动聚焦重要区域）\n",
    "        self.attention_pool = nn.Sequential(\n",
    "            nn.Linear(compress_dim, 1),  # 为每个patch生成注意力权重\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        # 颜色分类器\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(compress_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_cates))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 特征压缩 [8, 256, 512]\n",
    "        x = self.feature_fusion(x)\n",
    "        # 注意力权重计算 [8, 256, 1]\n",
    "        attn_weights = self.attention_pool(x)\n",
    "        # 加权聚合 [8, 512]\n",
    "        global_feature = torch.sum(x * attn_weights, dim=1)\n",
    "        # 颜色分类 [8, num_colors]\n",
    "        return torch.softmax(self.classifier(global_feature),-1)\n",
    "\n",
    "\n",
    "# color_classifier = GlobalClassifyingHead(2048).half().to(device)\n",
    "# cate_classifier = GlobalClassifyingHead(2048, num_cates=45).half().to(device)\n",
    "# color_weight = 0.2\n",
    "# cate_weight = 0.2\n",
    "# loss_weight = 1 - color_weight - cate_weight\n",
    "# if cp_epoch > 0:\n",
    "#     checkpoint_c = f'my_model/' + try_round + f'/color_classifier_epoch_{cp_epoch-1}_.pth'\n",
    "#     color_classifier.load_state_dict(torch.load(checkpoint_c))\n",
    "#     checkpoint_c = f'my_model/' + try_round + f'/cate_classifier_epoch_{cp_epoch-1}_.pth'\n",
    "#     cate_classifier.load_state_dict(torch.load(checkpoint_c))\n",
    "    \n",
    "lr = 5e-4\n",
    "# optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "loss_fn_output = torch.nn.CrossEntropyLoss().to(device)\n",
    "# loss_fn_color = torch.nn.CrossEntropyLoss().to(device)\n",
    "# loss_fn_cate = torch.nn.CrossEntropyLoss().to(device)\n",
    "# loss_fn = torch.nn.MSELoss().to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD([\n",
    "    {'params': filter(lambda p: p.requires_grad, model.parameters()), 'lr': lr},  # 冻结前层\n",
    "#     {'params': color_classifier.parameters(), 'lr': lr},\n",
    "#     {'params': cate_classifier.parameters(), 'lr': lr}\n",
    "])\n",
    "\n",
    "# optimizer = torch.optim.SGD(model.parameters(),lr=lr)\n",
    "\n",
    "\n",
    "# for m in color_classifier.modules():\n",
    "#     if isinstance(m, (nn.Conv2d)):\n",
    "#         nn.init.normal_(m.weight,mean=0,std=0.005)\n",
    "#     if isinstance(m, (nn.Linear)):\n",
    "#         nn.init.normal_(m.weight,mean=0,std=0.005)\n",
    "#         nn.init.normal_(m.bias,mean=0,std=0.005)\n",
    "\n",
    "epochs = 30\n",
    "max_tokens_to_generate = 100\n",
    "print('Start training on classifying colors:')\n",
    "print(f'Start at epoch {cp_epoch}')\n",
    "torch.cuda.empty_cache()\n",
    "# torch.cuda.memory._record_memory_history(max_entries=100000)\n",
    "for epoch in range(epochs):\n",
    "    epoch += cp_epoch\n",
    "    correct_num_train = 0\n",
    "    total_num_train = 0\n",
    "    loss_list = []\n",
    "    cate_list_pred, cate_list_true = [], []\n",
    "    color_list_pred, color_list_true = [], []\n",
    "    train_assessments = [] if epoch == 0 else pickle.load(open(f'track/'+try_round+'/train_assessments.pkl', 'rb'))\n",
    "    print('\\nShuffle training set')\n",
    "    training_set.shuffle(batch_size)  # shuffle training set\n",
    "    for i, (image_paths, prompts, label_words, images, colors, _, cates) in enumerate(training_set):\n",
    "#         if i == 10:\n",
    "#             break\n",
    "        images = [images[:, :, :, i] for i in range(images.shape[3])]\n",
    "        model_inputs = processor(prompts, images)\n",
    "        model_inputs = move_inputs_to_device(model_inputs, device)\n",
    "        input_ids = model_inputs['input_ids'].to(device)\n",
    "        attention_mask = model_inputs['attention_mask'].to(device)\n",
    "        pixel_values = model_inputs['pixel_values'].half().to(device)\n",
    "        kv_cache = KVCache()\n",
    "        labels_ids, labels_attention_mask, total_num = get_ids(tokenizer, label_words)\n",
    "        total_num_train += total_num\n",
    "        labels_ids = labels_ids.to(device)\n",
    "        labels_attention_mask = labels_attention_mask.to(device)\n",
    "        generated_words = ''\n",
    "        loss_list_sample = []\n",
    "        correct_num = 0\n",
    "        for j in range(labels_ids.shape[1]):\n",
    "            outputs, image_embeddings, image_features, hidden_feature = model.multi_forward(input_ids=input_ids,\n",
    "                                                                            pixel_values=pixel_values,\n",
    "                                                                            attention_mask=attention_mask,\n",
    "                                                                            kv_cache=kv_cache)\n",
    "            kv_cache = outputs[\"kv_cache\"]\n",
    "            word_ids = labels_ids[:, j].to(device)\n",
    "            word_vec = torch.zeros_like(outputs[\"logits\"][:, -1, :], dtype=torch.float16)\n",
    "            word_vec[list(range(batch_size)), word_ids[:, 0]] = 1.0\n",
    "            input_ids = word_ids.detach()  # 标签的下一个（一个batch）的字符作为下一次输入\n",
    "            attention_mask = torch.cat([attention_mask, (word_ids != 0).int()], dim=-1).detach().to(\n",
    "                device)  # 更新attention_mask，若batch中的某一条样本已经输入完，则mask对应的位置为0，否则为1\n",
    "\n",
    "#             if j == 0:\n",
    "#                 color_outputs = color_classifier(hidden_feature)\n",
    "#                 color_ids = list(color_outputs.argmax(dim=1).detach().cpu().numpy())\n",
    "#                 color_list_pred += color_ids\n",
    "#                 color_labels = [colors_dict[c] for c in colors]\n",
    "#                 color_list_true += color_labels\n",
    "#                 colors_vec = torch.eye(17)[color_labels].to(device)\n",
    "\n",
    "#                 cate_outputs = cate_classifier(hidden_feature)\n",
    "#                 cate_ids = list(cate_outputs.argmax(dim=1).detach().cpu().numpy())\n",
    "#                 cate_list_pred += cate_ids\n",
    "#                 cate_labels = [cate_dict[c] for c in cates]\n",
    "#                 cate_list_true += cate_labels\n",
    "#                 cate_vec = torch.eye(45)[cate_labels].to(device)\n",
    "\n",
    "#                 color_loss = loss_fn_color(color_outputs, colors_vec)\n",
    "#                 cate_loss = loss_fn_cate(cate_outputs, cate_vec)\n",
    "#                 optimizer.zero_grad()\n",
    "#                 if torch.isnan(color_loss) or torch.isnan(cate_loss):\n",
    "#                     print('nan')\n",
    "#             else:\n",
    "#                 optimizer.zero_grad()\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_fn_output(outputs[\"logits\"][:, -1, :], word_ids[:, 0])\n",
    "#             if j == 0:\n",
    "#                 final_loss = loss * loss_weight + color_loss * color_weight + cate_loss * cate_weight\n",
    "#                 final_loss.backward()\n",
    "#             else:\n",
    "#                 loss.backward()\n",
    "            loss.backward()\n",
    "            if torch.isnan(loss):\n",
    "                print('nan')\n",
    "            else:   \n",
    "                optimizer.step()\n",
    "            kv_cache.clear_trace()\n",
    "            loss = round(float(loss.detach().cpu()), 4)\n",
    "            pred_ids = torch.softmax(outputs[\"logits\"][:, -1, :], dim=-1, dtype=torch.float16).argmax(1)\n",
    "            ei = word_ids[:, 0] != 0\n",
    "            loss_list_sample.append(loss)\n",
    "#             printing = f'Epoch{epoch}\\tIter{i}\\t{j}\\tPredict Loss:{loss}\\t Color Loss:{color_loss}\\t Cate Loss:{cate_loss}'\n",
    "            printing = f'Epoch{epoch}\\tIter{i}\\t{j}\\tPredict Loss:{loss}'\n",
    "            print(printing, end='\\r')\n",
    "#             break\n",
    "            correct_num_train += (word_ids[:, 0][ei] == pred_ids[ei]).sum().item()\n",
    "        loss_list.append(sum(loss_list_sample) / len(loss_list_sample))\n",
    "        torch.cuda.empty_cache()\n",
    "#         break\n",
    "#     train_assessments.append([float(correct_num_train / total_num_train),\n",
    "#                       accuracy_score(color_list_true, color_list_pred),\n",
    "#                       f1_score(color_list_true, color_list_pred, average='macro'),\n",
    "#                       accuracy_score(cate_list_true, cate_list_pred),\n",
    "#                       f1_score(cate_list_true, cate_list_pred, average='macro')])\n",
    "\n",
    "    train_assessments.append(float(correct_num_train / total_num_train))\n",
    "    print('Saving loss, training assessments and the model...')\n",
    "    torch.save(model.state_dict(),\n",
    "               f'my_model/{try_round}/model_rank{rank}_epoch_{epoch}_.pth')\n",
    "#     torch.save(color_classifier.state_dict(),\n",
    "#                f'my_model/{try_round}/color_classifier_epoch_{epoch}_.pth')\n",
    "#     torch.save(cate_classifier.state_dict(),\n",
    "#                f'my_model/{try_round}/cate_classifier_epoch_{epoch}_.pth')\n",
    "    with open(f'track/{try_round}/loss_rank{rank}_epoch_{epoch}_.pkl', 'wb') as f:\n",
    "        pickle.dump(loss_list, f)\n",
    "    with open(f'track/{try_round}/train_assessments.pkl', 'wb') as f:\n",
    "        pickle.dump(train_assessments, f)\n",
    "\n",
    "    validation_acc = [] if epoch == 0 else pickle.load(open(f'track/'+try_round+'/validation_acc.pkl', 'rb'))\n",
    "    with torch.no_grad():\n",
    "        print(f'Validating after epoch {epoch}')\n",
    "        correct_num_valid = 0\n",
    "        total_num_valid = 0\n",
    "        validation_set.shuffle(4)\n",
    "        for i, (image_paths, prompts, label_words, images, _, _, _) in enumerate(validation_set):\n",
    "            print(f'\\r{i}/{len(validation_set)}', end='')\n",
    "            images = [images[:, :, :, i] for i in range(images.shape[3])]\n",
    "            model_inputs = processor(prompts, images)\n",
    "            model_inputs = move_inputs_to_device(model_inputs, device)\n",
    "            input_ids = model_inputs['input_ids'].to(device)\n",
    "            attention_mask = model_inputs['attention_mask'].to(device)\n",
    "            pixel_values = model_inputs['pixel_values'].half().to(device)\n",
    "            kv_cache = KVCache()\n",
    "            labels_ids, labels_attention_mask, total_num = get_ids(tokenizer, label_words)\n",
    "            total_num_valid += total_num\n",
    "            labels_ids = labels_ids.to(device)\n",
    "            labels_ids[labels_ids==0] = -1\n",
    "            labels_attention_mask = labels_attention_mask.to(device)\n",
    "            for j in range(max_tokens_to_generate):\n",
    "                if j >= labels_ids.shape[1]:\n",
    "                    break\n",
    "                outputs, _, _ = model(input_ids=input_ids,\n",
    "                                        pixel_values=pixel_values,\n",
    "                                        attention_mask=attention_mask,\n",
    "                                        kv_cache=kv_cache)\n",
    "                kv_cache = outputs[\"kv_cache\"]\n",
    "                pred_ids = outputs[\"logits\"][:, -1, :].argmax(1)\n",
    "                attention_mask = torch.cat([attention_mask, (labels_ids[:, j, :] != -1).int()], dim=-1).detach().to(device)\n",
    "                correct_num_valid += (pred_ids == labels_ids[:, j, 0]).sum()\n",
    "                input_ids = torch.unsqueeze(pred_ids, 1)\n",
    "    val_acc = float((correct_num_valid/total_num_valid).cpu().numpy())\n",
    "    validation_acc.append(val_acc)\n",
    "    print(f'\\n\\tAccuracy on validation set is {round(val_acc, 4)}')\n",
    "    print('\\tSaving accuracy on validation set\\n')\n",
    "    with open(f'track/{try_round}/validation_acc.pkl', 'wb') as f:\n",
    "        pickle.dump(validation_acc, f)\n",
    "    training_set, validation_set = prepare_dataset(bs=batch_size)\n",
    "\n",
    "# torch.cuda.memory._dump_snapshot(f\"track/rank_{rank}_bit_{bit}_GPUMemoryUsage.pickle\")\n",
    "# torch.cuda.memory._record_memory_history(enabled=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1a4bae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d290bf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.memory._dump_snapshot(\"test.pickle\")\n",
    "torch.cuda.memory._record_memory_history(enabled=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1909191b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.5352, -0.0342,  0.0956,  ...,  0.0394,  0.2649,  0.0719],\n",
       "        [ 0.1616, -0.1614, -0.1450,  ..., -0.0263,  0.0057, -0.0356],\n",
       "        [ 0.1212,  0.0206, -0.0285,  ..., -0.0060,  0.0020, -0.0056],\n",
       "        ...,\n",
       "        [ 0.3709, -0.3123,  0.1329,  ...,  0.4703, -0.0135, -0.0267],\n",
       "        [-0.3849,  0.2765, -0.4246,  ...,  0.3767,  0.2968,  0.1435],\n",
       "        [-0.0473,  0.4165, -0.3479,  ...,  0.0354,  0.1006, -0.0609]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.language_model.base_model.model.model.embed_tokens.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6d3dbbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.1900e-25, -1.4440e-24, -2.6146e-25,  ...,  8.0674e-25,\n",
       "          6.8459e-26, -2.4822e-25],\n",
       "        [ 2.3831e-01,  4.2277e-01,  1.1893e-01,  ...,  6.3680e-02,\n",
       "          3.8681e-02, -2.3741e-02],\n",
       "        [-2.5610e-41, -9.7076e-41, -6.1769e-42,  ...,  6.5886e-41,\n",
       "          8.8114e-42, -2.0620e-41],\n",
       "        ...,\n",
       "        [-2.3259e-34, -6.8676e-34, -5.2102e-34,  ...,  9.0979e-35,\n",
       "         -8.0793e-35,  6.3096e-35],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00]], device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.language_model.base_model.model.model.embed_tokens.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15ac32d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
       "        [nan, nan, nan,  ..., nan, nan, nan],\n",
       "        [nan, nan, nan,  ..., nan, nan, nan],\n",
       "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[\"logits\"][:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e5f9be1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[257152, 257152, 257152,  ...,      0,      0,      0],\n",
       "        [257152, 257152, 257152,  ...,      0,      0,      0],\n",
       "        [257152, 257152, 257152,  ...,      0,      0,      0],\n",
       "        [257152, 257152, 257152,  ...,      0,      0,      0]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba98fb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 262875\n",
    "# 262888\n",
    "# 257153\n",
    "# 262536\n",
    "# 262852\n",
    "# 262301"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57c5aa9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30888"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(262301-257153)*6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7324987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data augmenting...\n",
      "35551/35552\n",
      "Viewing the set into batch_size=4\n",
      "Data augmenting...\n",
      "4443/4444\n",
      "Viewing the set into batch_size=4\n",
      "Viewing the set into batch_size=1\n",
      "Viewing the set into batch_size=1\n",
      "Update tokenizer\n",
      "4443\n",
      "Update tokenizer\n",
      "35551\n"
     ]
    }
   ],
   "source": [
    "'''更新tokenizer'''\n",
    "training_set, validation_set = prepare_dataset(validation=1)\n",
    "training_set.shuffle(1)\n",
    "validation_set.shuffle(1)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "update_tokenizer(tokenizer, validation_set)\n",
    "update_tokenizer(tokenizer, training_set)\n",
    "# update_tokenizer(tokenizer, testing_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "252a435c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('paligemma/tokenizer_config.json',\n",
       " 'paligemma/special_tokens_map.json',\n",
       " 'paligemma/tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8dd8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "lr = 1e-4\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=lr, eps=1e-3)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "loss_fn = torch.nn.CrossEntropyLoss().to(device)\n",
    "epochs = 50\n",
    "max_tokens_to_generate = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe19fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "整体LoRA微调\n",
    "'''\n",
    "try_round = 3\n",
    "print('Start training:')\n",
    "print(f'Start at epoch {cp_epoch}')\n",
    "for epoch in range(epochs):\n",
    "    epoch += cp_epoch\n",
    "    training_set, validation_set = prepare_dataset(validation=1)\n",
    "    correct_num_train = 0\n",
    "    total_num_train = 0\n",
    "    loss_list = []\n",
    "    train_acc = [] if epoch == 0 else pickle.load(open(f'track/{try_round}/train_acc.pkl', 'rb'))\n",
    "    print('\\nShuffle training set')\n",
    "    training_set.shuffle(batch_size)  # shuffle training set\n",
    "    for i, (image_paths, prompts, label_words, images, _, _, _) in enumerate(training_set):\n",
    "        images = [images[:, :, :, i] for i in range(images.shape[3])]\n",
    "        model_inputs = processor(prompts, images)\n",
    "        model_inputs = move_inputs_to_device(model_inputs, device)\n",
    "        input_ids = model_inputs['input_ids'].to(device)\n",
    "        attention_mask = model_inputs['attention_mask'].to(device)\n",
    "        pixel_values = model_inputs['pixel_values'].half().to(device)\n",
    "        kv_cache = KVCache()\n",
    "        labels_ids, labels_attention_mask, total_num = get_ids(tokenizer, label_words)\n",
    "        labels_ids = labels_ids.to(device)\n",
    "        labels_attention_mask = labels_attention_mask.to(device)\n",
    "        generated_words = ''\n",
    "        loss_list_sample = []\n",
    "        correct_num = 0\n",
    "        for j in range(labels_ids.shape[1]):\n",
    "            outputs, _, _ = model(\n",
    "                input_ids=input_ids,\n",
    "                pixel_values=pixel_values,\n",
    "                attention_mask=attention_mask,\n",
    "                kv_cache=kv_cache, )\n",
    "            kv_cache = outputs[\"kv_cache\"]\n",
    "            word_ids = labels_ids[:, j].to(device)\n",
    "            word_vec = torch.zeros_like(outputs[\"logits\"][:, -1, :], dtype=torch.float16)\n",
    "            word_vec[list(range(batch_size)), word_ids[:, 0]] = 1.0\n",
    "            input_ids = word_ids.detach()  # 标签的下一个（一个batch）的字符作为下一次输入\n",
    "            attention_mask = torch.cat([attention_mask, (word_ids != 0).int()], dim=-1).detach().to(\n",
    "                device)  # 更新attention_mask，若batch中的某一条样本已经输入完，则mask对应的位置为0，否则为1\n",
    "\n",
    "            loss = loss_fn(outputs[\"logits\"][:, -1, :], word_ids[:, 0])\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            kv_cache.clear_trace()\n",
    "            loss = loss.item()\n",
    "            pred_ids = torch.softmax(outputs[\"logits\"][:, -1, :], dim=-1, dtype=torch.float16).argmax(1)\n",
    "            ei = word_ids[:, 0] != 0\n",
    "            correct_num += (word_ids[:, 0][ei] == pred_ids[ei]).sum().item()\n",
    "            loss_list_sample.append(loss)\n",
    "            print(f'epoch{epoch}\\t{i}\\tloss: {round(loss, 4)}', end='\\r')\n",
    "        #         print(f'\\n\\tacc:{round(correct_num / total_num, 4)}\\ttokens to predict:{total_num}\\tcorrect:{correct_num}')\n",
    "        total_num_train += total_num\n",
    "        correct_num_train += correct_num\n",
    "        loss_list.append(sum(loss_list_sample) / len(loss_list_sample))\n",
    "        torch.cuda.empty_cache()\n",
    "    train_acc.append(correct_num_train / total_num_train)\n",
    "    print('Saving loss, training accuracy and the model...')\n",
    "    torch.save(model.state_dict(),\n",
    "               f'my_model/{try_round}/model_{str(datetime.now().date())}_rank{rank}_epoch_{epoch}_.pth')\n",
    "    with open(f'track/{try_round}/loss_{str(datetime.now().date())}_rank{rank}_epoch_{epoch}_.pkl', 'wb') as f:\n",
    "        pickle.dump(loss_list, f)\n",
    "    with open(f'track/{try_round}/train_acc.pkl', 'wb') as f:\n",
    "        pickle.dump(train_acc, f)\n",
    "\n",
    "    validation_acc = [] if epoch == 0 else pickle.load(open(f'track/{try_round}/validation_acc.pkl', 'rb'))\n",
    "    with torch.no_grad():\n",
    "        print(f'Validating after epoch {epoch}')\n",
    "        correct_num_valid = 0\n",
    "        total_num_valid = 0\n",
    "        validation_set.shuffle(4)\n",
    "        for i, (image_paths, prompts, label_words, images, _, _, _) in enumerate(validation_set):\n",
    "            print(f'\\r{i}/{len(validation_set)}', end='')\n",
    "            images = [images[:, :, :, i] for i in range(images.shape[3])]\n",
    "            model_inputs = processor(prompts, images)\n",
    "            model_inputs = move_inputs_to_device(model_inputs, device)\n",
    "            input_ids = model_inputs['input_ids'].to(device)\n",
    "            attention_mask = model_inputs['attention_mask'].to(device)\n",
    "            pixel_values = model_inputs['pixel_values'].half().to(device)\n",
    "            kv_cache = KVCache()\n",
    "            labels_ids, labels_attention_mask, total_num = get_ids(tokenizer, label_words)\n",
    "            total_num_valid += total_num\n",
    "            labels_ids = labels_ids.to(device)\n",
    "            #         labels_ids[labels_ids==0] -= 1\n",
    "            labels_attention_mask = labels_attention_mask.to(device)\n",
    "            for j in range(max_tokens_to_generate):\n",
    "                if j >= labels_ids.shape[1]:\n",
    "                    break\n",
    "                #             total_num_valid += 4 - (labels_ids[:,j,0] == -1).sum()\n",
    "                outputs, _, _ = model(\n",
    "                    input_ids=input_ids,\n",
    "                    pixel_values=pixel_values,\n",
    "                    attention_mask=attention_mask,\n",
    "                    kv_cache=kv_cache, )\n",
    "                kv_cache = outputs[\"kv_cache\"]\n",
    "                pred_ids = outputs[\"logits\"][:, -1, :].argmax(1)\n",
    "                #             attention_mask = torch.cat([attention_mask, torch.tensor([[1]]).to(device)], dim=-1).to(device)\n",
    "                attention_mask = torch.cat([attention_mask, (labels_ids[:, j, :] != 0).int()], dim=-1).detach().to(\n",
    "                    device)\n",
    "                correct_num_valid += (pred_ids == labels_ids[:, j, 0]).sum()\n",
    "                input_ids = torch.unsqueeze(pred_ids, 1)\n",
    "    #             if pred_ids == stop_token:\n",
    "    #                 break\n",
    "    acc_valid = float(correct_num_valid / total_num_valid)\n",
    "    print(f'\\nAccuracy on validation set is {round(acc_valid, 4)}\\n')\n",
    "    validation_acc.append(acc_valid)\n",
    "    print('Saving accuracy on validation set\\n')\n",
    "    with open(f'track/{try_round}/validation_acc.pkl', 'wb') as f:\n",
    "        pickle.dump(validation_acc, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c762efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "training_set, validation_set = prepare_dataset(bs=batch_size)\n",
    "colors_dict = list(np.unique(training_set.baseColour))\n",
    "colors_dict = {color: i for i, color in enumerate(colors_dict)}\n",
    "\n",
    "cate_dict = list(np.unique(training_set.subCategory))\n",
    "cate_dict = {cate: i for i, cate in enumerate(cate_dict)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c42256",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, m in model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94652e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "加强颜色感知\n",
    "'''\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
    "\n",
    "\n",
    "class Reshape(nn.Module):\n",
    "    def __init__(self, shape, exchange_dims):\n",
    "        super(Reshape, self).__init__()\n",
    "        self.shape = shape\n",
    "        self.exchange_dims = exchange_dims\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(self.shape).swapdims(self.exchange_dims[0], self.exchange_dims[1])\n",
    "\n",
    "\n",
    "# checkpoint = f'my_model/2/model_2025-02-27_rank16_epoch_25_.pth'\n",
    "# model.load_state_dict(torch.load(checkpoint))\n",
    "\n",
    "for name, m in model.named_parameters():  # 只更新后两层\n",
    "    if (\n",
    "            'vision_tower.vision_model.encoder.layers.26' in name or 'vision_tower.vision_model.encoder.layers.25' in name or 'post_layernorm' in name) and 'original' not in name:\n",
    "        m.requires_grad = True\n",
    "    else:\n",
    "        m.requires_grad = False\n",
    "\n",
    "lr = 1e-3\n",
    "# optimizer = torch.optim.Adam(, lr=lr, eps=1e-3)\n",
    "\n",
    "\n",
    "# optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "loss_fn = torch.nn.CrossEntropyLoss().to(device)\n",
    "# loss_fn = torch.nn.MSELoss().to(device)\n",
    "\n",
    "epochs = 30\n",
    "max_tokens_to_generate = 100\n",
    "\n",
    "\n",
    "class GlobalColorHead(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_dim=1152,  # 输入特征维度\n",
    "                 compress_dim=512,  # 特征压缩维度\n",
    "                 num_colors=17):  # 颜色类别数\n",
    "        super().__init__()\n",
    "        # 特征压缩与全局融合\n",
    "        self.feature_fusion = nn.Sequential(\n",
    "            #             nn.LayerNorm(in_dim),\n",
    "            nn.Linear(in_dim, compress_dim),  # 降维减少计算量\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        # 注意力聚合模块（自动聚焦重要区域）\n",
    "        self.attention_pool = nn.Sequential(\n",
    "            nn.Linear(compress_dim, 1),  # 为每个patch生成注意力权重\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        # 颜色分类器\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(compress_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_colors))\n",
    "\n",
    "    def forward(self, x):\n",
    "        #         B, H, W, C = x.shape\n",
    "        #         x = x.view(B, H*W, C)  # [8, 256, 1152]\n",
    "        # 特征压缩 [8, 256, 512]\n",
    "        x = self.feature_fusion(x)\n",
    "        # 注意力权重计算 [8, 256, 1]\n",
    "        attn_weights = self.attention_pool(x)\n",
    "        # 加权聚合 [8, 512]\n",
    "        global_feature = torch.sum(x * attn_weights, dim=1)\n",
    "        # 颜色分类 [8, num_colors]\n",
    "        return self.classifier(global_feature)\n",
    "\n",
    "\n",
    "color_classifier = GlobalColorHead().half().to(device)\n",
    "if cp_epoch > 0:\n",
    "    checkpoint_c = f'my_model/' + try_round + '/color_classifier.pth'\n",
    "    model.load_state_dict(torch.load(checkpoint_c))\n",
    "\n",
    "optimizer = torch.optim.SGD([\n",
    "    {'params': filter(lambda p: p.requires_grad, model.parameters()), 'lr': 1e-2},  # 冻结前层\n",
    "    {'params': color_classifier.parameters(), 'lr': 1e-2}\n",
    "])\n",
    "\n",
    "# for m in color_classifier.modules():\n",
    "#     if isinstance(m, (nn.Conv2d)):\n",
    "#         nn.init.normal_(m.weight,mean=0,std=0.005)\n",
    "#     if isinstance(m, (nn.Linear)):\n",
    "#         nn.init.normal_(m.weight,mean=0,std=0.005)\n",
    "#         nn.init.normal_(m.bias,mean=0,std=0.005)\n",
    "\n",
    "\n",
    "print('Start training on classifying colors:')\n",
    "print(f'Start at epoch {cp_epoch}')\n",
    "for epoch in range(epochs):\n",
    "    epoch += cp_epoch\n",
    "    correct_num_train = 0\n",
    "    total_num_train = 0\n",
    "    pred_list_train = []\n",
    "    true_list_train = []\n",
    "    train_acc = [] if epoch == 0 else pickle.load(open(f'track/' + str(try_round) + '/train_acc.pkl', 'rb'))\n",
    "    train_f1 = [] if epoch == 0 else pickle.load(open(f'track/' + str(try_round) + '/train_f1.pkl', 'rb'))\n",
    "    training_set.shuffle(batch_size)\n",
    "    for i, (image_paths, prompts, label_words, images, colors, _, _) in enumerate(training_set):\n",
    "        images = [images[:, :, :, i] for i in range(images.shape[3])]\n",
    "        model_inputs = processor(prompts, images)\n",
    "        model_inputs = move_inputs_to_device(model_inputs, device)\n",
    "        input_ids = model_inputs['input_ids'].to(device)\n",
    "        pixel_values = model_inputs['pixel_values'].half().to(device)\n",
    "        image_features = model.distract_image_features(\n",
    "            input_ids=input_ids,\n",
    "            pixel_values=pixel_values)\n",
    "        outputs = color_classifier(image_features)\n",
    "        outputs_ids = outputs.argmax(dim=1).detach().cpu().numpy()\n",
    "        colors_ids = [colors_dict[c] for c in colors]\n",
    "        colors_vec = torch.eye(17)[colors_ids].to(device)\n",
    "        loss = loss_fn(outputs.to(torch.float32), colors_vec)\n",
    "        optimizer.zero_grad()\n",
    "        #         t = torch.sum(image_features)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss = float(loss.detach().cpu())\n",
    "        print(f'epoch{epoch}\\t{i}\\tloss: {round(loss, 4)}', end='\\r')\n",
    "        for c in colors_ids:\n",
    "            true_list_train.append(c)\n",
    "        for c in outputs_ids:\n",
    "            pred_list_train.append(c)\n",
    "        torch.cuda.empty_cache()\n",
    "    acc = accuracy_score(true_list_train, pred_list_train)\n",
    "    f1 = f1_score(true_list_train, pred_list_train, average='macro')\n",
    "    train_acc.append(acc)\n",
    "    train_f1.append(f1)\n",
    "    print(f'\\nAccuracy on training set: {round(acc, 4)}\\nF1 score on training set: {round(f1, 4)}')\n",
    "    print('Saving loss, training accuracy and the model...\\n')\n",
    "    torch.save(model.state_dict(),\n",
    "               f'my_model/' + str(try_round) + f'/model_{str(datetime.now().date())}_rank{rank}_epoch_{epoch}_.pth')\n",
    "    torch.save(color_classifier.state_dict(), f'checkpoint_c')\n",
    "    with open(f'track/' + str(try_round) + '/train_acc.pkl', 'wb') as f:\n",
    "        pickle.dump(train_acc, f)\n",
    "    with open(f'track/' + str(try_round) + '/train_f1.pkl', 'wb') as f:\n",
    "        pickle.dump(train_f1, f)\n",
    "\n",
    "    validation_acc = [] if epoch == 0 else pickle.load(open(f'track/' + str(try_round) + '/validation_acc.pkl', 'rb'))\n",
    "    validation_f1 = [] if epoch == 0 else pickle.load(open(f'track/' + str(try_round) + '/validation_f1.pkl', 'rb'))\n",
    "    validation_set.shuffle(batch_size)\n",
    "    with torch.no_grad():\n",
    "        print(f'Validating after epoch {epoch}')\n",
    "        true_list_val = []\n",
    "        pred_list_val = []\n",
    "        for i, (image_paths, prompts, label_words, images, colors, _, _) in enumerate(validation_set):\n",
    "            print(f'\\r{i}/{len(validation_set)}', end='')\n",
    "            images = [images[:, :, :, i] for i in range(images.shape[3])]\n",
    "            model_inputs = processor(prompts, images)\n",
    "            model_inputs = move_inputs_to_device(model_inputs, device)\n",
    "            input_ids = model_inputs['input_ids'].to(device)\n",
    "            pixel_values = model_inputs['pixel_values'].half().to(device)\n",
    "            image_features = model.distract_image_features(input_ids=input_ids,\n",
    "                                                           pixel_values=pixel_values)\n",
    "            outputs = color_classifier(image_features)\n",
    "            outputs_ids = outputs.argmax(dim=1).detach().cpu().numpy()\n",
    "            colors_ids = [colors_dict[c] for c in colors]\n",
    "            for c in colors_ids:\n",
    "                true_list_val.append(c)\n",
    "            for c in outputs_ids:\n",
    "                pred_list_val.append(c)\n",
    "    acc = accuracy_score(true_list_val, pred_list_val)\n",
    "    f1 = f1_score(true_list_val, pred_list_val, average='macro')\n",
    "    validation_acc.append(acc)\n",
    "    validation_f1.append(f1)\n",
    "    print(f'\\nAccuracy on validation set: {round(acc, 4)}\\nF1 score on validation set: {round(f1, 4)}')\n",
    "    print('Saving accuracy and f1 score on validation set\\n' + '*' * 60)\n",
    "    with open(f'track/' + str(try_round) + '/validation_acc.pkl', 'wb') as f:\n",
    "        pickle.dump(validation_acc, f)\n",
    "    with open(f'track/' + str(try_round) + '/validation_f1.pkl', 'wb') as f:\n",
    "        pickle.dump(validation_f1, f)\n",
    "    training_set, validation_set = prepare_dataset(validation=1)\n",
    "#     print('\\nShuffle training set')\n",
    "#     training_set.shuffle(batch_size)  # shuffle training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afb16b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3e01bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f299aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_classifier.feature_fusion[0].weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b943da80",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = model.vision_tower.vision_model.encoder.layers[26].self_attn.v_proj.parametrizations.weight[0].lora_B\n",
    "m.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eaff89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.vision_tower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f94017",
   "metadata": {},
   "outputs": [],
   "source": [
    "m - torch.tensor(model.vision_tower.vision_model.encoder.layers[25].self_attn.k_proj.parametrizations.weight[0].lora_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c32eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_acc = [0.6005, 0.6156, 0.6094, 0.6152, 0.6028]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0442b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'track/{try_round}/validation_acc.pkl', 'wb') as f:\n",
    "    pickle.dump(validation_acc, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b355ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_num_valid / total_num_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5032e0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Distracting features...')\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "colors_list = []\n",
    "cate_list = []\n",
    "features_mat = []\n",
    "epochs = 10\n",
    "bs = 1\n",
    "with torch.no_grad():\n",
    "    # for epoch in range(epochs):\n",
    "    training_set.shuffle(1)  # shuffle training set\n",
    "    #     small_set = training_set.sample(list(range(3000)))\n",
    "    #     small_set.shuffle(bs)\n",
    "    #     tot = int(len(small_set))\n",
    "    tot = int(len(training_set))\n",
    "    for i, (image_paths, prompts, label_words, images) in enumerate(training_set):\n",
    "        colors = training_set.baseColour[training_set.inds[i]]\n",
    "        cates = training_set.subCategory[training_set.inds[i]]\n",
    "        if colors[0] != 'Green' and colors[0] != 'Grey':\n",
    "            continue\n",
    "        torch.cuda.empty_cache()\n",
    "        images = [images[:, :, :, i] for i in range(images.shape[3])]\n",
    "        model_inputs = processor(prompts, images)\n",
    "        model_inputs = move_inputs_to_device(model_inputs, device)\n",
    "        input_ids = model_inputs['input_ids'].to(device)\n",
    "        pixel_values = model_inputs['pixel_values'].half().to(device)\n",
    "\n",
    "        features = model.distract_image_features(\n",
    "            input_ids=input_ids,\n",
    "            pixel_values=pixel_values)\n",
    "        for j in range(bs):\n",
    "            cate_list.append(cates[j])\n",
    "            colors_list.append(colors[j])\n",
    "            features_mat.append(torch.ravel(features[j]).cpu().numpy())\n",
    "        print(f'{i}/{tot}', end='\\r')\n",
    "features_mat = np.array(features_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef5bded",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca.fit(features_mat)\n",
    "features_PCA = pca.transform(features_mat)\n",
    "pca.explained_variance_ratio_.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093c5216",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_drew = {}\n",
    "for color in set(colors_list):\n",
    "    color_drew[color] = np.random.random((1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d56fbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data/fashion-dataset/features_PCA.npy', features_PCA)\n",
    "with open(f'data/fashion-dataset/colors_list.pkl', 'wb') as f:\n",
    "    pickle.dump(colors_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f214836a",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_drew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d2d647",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "# ax = plt.axes(projection=\"3d\")\n",
    "for i in range(features_PCA.shape[0]):\n",
    "    point = features_PCA[i]\n",
    "    color = colors_list[i]\n",
    "    #     ax.scatter3D(point[0], point[1], point[2], color='b' if color=='Blue' else 'k')\n",
    "    plt.scatter(point[0], point[1], c='g' if color == 'Green' else 'grey', s=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fd5ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "size = len(colors_list)\n",
    "t_size = int(size * 0.8)\n",
    "clf = svm.SVC(C=1, kernel='rbf', gamma=0.1)\n",
    "clf.fit(features_PCA[:t_size], colors_list[:t_size])\n",
    "pred = clf.predict(features_PCA[t_size:])\n",
    "(np.array(pred) == np.array(colors_list[t_size:])).sum() / (size - t_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
